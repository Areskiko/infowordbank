# Word Bank for Information Theory

Or everything you need to know for the exam

# Applications in Compression

> What is the minimum space needed for some information

## Information in Bits

A measure of the information content of an event

## Event - X

A stochastic variable `X` is a set of events which all have a chance of
occurring with some probability `p_i` for `i ∈ |X|` where `|X|` denotes the
cardinality of the alphabet (the number of events which may occur for the
stochastic variable)

## Entropy - H(X)

Entropy is defined as `H(X)=-∑p_i⋅log(p_i)` where `p_i` is the probability of
some event `i`, entropy is positive. In layman's terms it is the average
information of any event divided by the uncertainty

### Binary Entropy

For a binary stochastic variable `X`

### Shannon information - I(p)

`I(p)=-log_2(p)` gives the information in bits
`I(p)=-log_e(p)` gives the information in nats

# Applications in Communication

> How fast can we transmit data such that we do not lose information

## Signal to noise ratio - SNR

Signal divided by the noise
